{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= open(\"my_corpus.txt\",encoding= \"utf8\")\n",
    "#file= open(\"test.ig.txt\",encoding= \"utf8\")\n",
    "\n",
    "filex= file.read()\n",
    "#filet= filex.lower()\n",
    "#filet= re.sub(r\"[^A-Za-z àáéèọíìịị̀ụụ́ụ̀úù]\",'',filet)#Use regex to clean sentence\n",
    "file= filex.split()\n",
    "filex= file[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning My Dataset\n",
    "\n",
    "The codes below perform the ffg funtion\n",
    "\n",
    "1. cleaner(): Removes symbols and use lower()\n",
    "2. remove_rare_words(): Remove words with less than one occurence\n",
    "3. removes spaces in the text using .strip().replace(\"  \",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neat version of the code without the tests\n",
    "\n",
    "# CHATGPT code to remove one_word occurence\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def remove_rare_words(corpus):\n",
    "    # Split the corpus into sentences\n",
    "    #sentences = re.split(r'[.!?]', corpus)\n",
    "    sentences = corpus.split('\\n')\n",
    "    #sentences = corpus\n",
    "    \n",
    "    \n",
    "    # Create a Counter object to count the occurrences of each word\n",
    "    word_counts = Counter()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.split():\n",
    "            word_counts[word] += 1\n",
    "            \n",
    "    #print (word_counts)\n",
    "    # Remove sentences that contain words that occur only once\n",
    "    #new_corpus = \"\"\n",
    "    new_corpus=[]\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        include_sentence = True\n",
    "        for word in sentence.split():\n",
    "            #if word_counts[word] == 1:\n",
    "            if word_counts[word] <= 2:\n",
    "                include_sentence = False\n",
    "                break\n",
    "        if include_sentence:\n",
    "            #new_corpus += sentence + \" \" \n",
    "            new_corpus.append(sentence)\n",
    "    \n",
    "        \n",
    "    #return new_corpus.strip()\n",
    "    return \"\\n\".join(new_corpus),word_counts # returning the \"x\".strip().replace(\"  \",\" \") to remove extra spaces does not work when used within the function\n",
    "    \n",
    "\n",
    "#corpus = \"This is a sentence. This is another sentence. This is a unique sentence.\"\n",
    "#print(remove_rare_words(corpus))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'half_clean_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-18ce77c03357>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Assuming half_clean_file and non_igbo are defined properly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mclean_file_of_non_igbo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_words_that_are_not_igbo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhalf_clean_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_igbo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'half_clean_file' is not defined"
     ]
    }
   ],
   "source": [
    "def remove_words_that_are_not_igbo(corpus, non_igwrds):\n",
    "    sentences = corpus.split('\\n')\n",
    "    non_igbo_words = set(non_igwrds.split('\\n'))\n",
    "\n",
    "    new_corpus = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        include_sentence = all(word not in non_igbo_words for word in sentence.split())\n",
    "        if include_sentence:\n",
    "            new_corpus.append(sentence)\n",
    "\n",
    "    return \"\\n\".join(new_corpus)\n",
    "\n",
    "# Assuming half_clean_file and non_igbo are defined properly\n",
    "clean_file_of_non_igbo = remove_words_that_are_not_igbo(half_clean_file, non_igbo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open jw300.ig.text file for cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\\njob\\niv\\ndaniel\\nbetel\\nunited\\nstates\\nnew\\nof\\nmat\\nsamuel\\ngris\\nnizrel\\nrut\\nisrael\\ngermany\\ndr\\njohn\\nmap\\nspe'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file= open(\"my_corpus.txt\", encoding=\"utf8\")\n",
    "sent= file.read()\n",
    "senta=sent[:10000]\n",
    "#file= open(\"test.ig.txt\", encoding=\"utf8\")\n",
    "#file= open(\"text_file_modified.txt\", encoding=\"utf8\")\n",
    "\n",
    "\n",
    "\n",
    "file= open(\"non_Igbo_words_in_non_conform.txt\", encoding=\"utf8\")\n",
    "non_igbo= file.read()\n",
    "non_igbo[:100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_clean_file, diction= remove_rare_words(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_file_of_non_igbo = remove_words_that_are_not_igbo(half_clean_file,non_igbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375303"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#half_clean_file[:100]\n",
    "#diction\n",
    "\n",
    "# 46,213 sentences with\n",
    "\n",
    "#len(sent.split('\\n'))#375303\n",
    "#len(half_clean_file.split('\\n'))#361335\n",
    "#len(clean_file_of_non_igbo.split('\\n'))#329090\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iji ubube eme ihe ì na eme nnyocha ndị a iji zere mmerụ ahụ \\nsite naka onye nta akụkọ teta \\nọ dịkwa '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#half_clean_file, diction= remove_rare_words(cleaner(sent)) \n",
    "\n",
    "#half_clean_file2, diction= cleaner(remove_rare_words(sent))\n",
    "\n",
    "\n",
    "Cleaned_file= clean_file_of_non_igbo.strip().replace(\"  \",\" \")\n",
    "Cleaned_file[:100]\n",
    "\n",
    "#Cleaned_file_araba= clean_file_for_araba_data.strip().replace(\"  \",\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= open('my_corpus_with_my_non_igbo_words.txt', 'w+', encoding='utf8')#half_clean_file\n",
    "\n",
    "file.write(Cleaned_file)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
