{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5320"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "my_file_e= open(\"English_translation_of Words_to_be_reviewed.txt\",encoding = \"utf8\")\n",
    "my_file_e= my_file_e.read()\n",
    "my_file_e=re.sub(r\"[^\\w\\s’]\",'',my_file_e)\n",
    "my_file_e= my_file_e.replace(\" \",\"\")\n",
    "my_file_e= my_file_e.split('\\n')\n",
    "#my_file_e= my_file_e[:10]#\n",
    "len(my_file_e)\n",
    "#my_file_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5320"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_file= open(\"words_tbr_python.txt\",encoding = \"utf8\")\n",
    "my_file= my_file.read()\n",
    "my_file=re.sub(r\"[^\\w\\s’]\",'',my_file)\n",
    "my_file= my_file.replace(\" \",\"\")\n",
    "my_file= my_file.split('\\n')\n",
    "#my_file= my_file[:10]#\n",
    "len(my_file)\n",
    "#my_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "igbo=[]\n",
    "non_igbo=[]\n",
    "for i in range(len(my_file)):\n",
    "    if my_file[i] != my_file_e[i].lower():\n",
    "        igbo.append(my_file[i])\n",
    "    else:\n",
    "        non_igbo.append(my_file[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3972"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(igbo)\n",
    "len(non_igbo)\n",
    "#non_igbo\n",
    "#igbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "igbo= \"\\n\".join(igbo)\n",
    "non_igbo= \"\\n\".join(non_igbo)\n",
    "\n",
    "file= open('Igbo_words_in_non_conform.txt', 'w+', encoding='utf8')\n",
    "file.write(igbo)\n",
    "file.close()\n",
    "\n",
    "file= open('non_Igbo_words_in_non_conform.txt', 'w+', encoding='utf8')\n",
    "file.write(non_igbo)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Words I will need to add to the non_igbo words to be removed\n",
    "These words are in the Igbo_words present in non_conform\n",
    "\n",
    "- dollar\n",
    "- erad\n",
    "- today\n",
    "- John\n",
    "Japan\n",
    "south\n",
    "jehovahs\n",
    "nataniel\n",
    "ruben\n",
    "kgb\n",
    "het\n",
    "ernest\n",
    "gold\n",
    "copt\n",
    "von\n",
    "boyd\n",
    "sin\n",
    "yury\n",
    "tulun\n",
    "vitaly\n",
    "hardaker\n",
    "stein\n",
    "twain\n",
    "waring\n",
    "hart\n",
    "gray\n",
    "galen\n",
    "inch\n",
    "photograph\n",
    "meyer\n",
    "mankinds\n",
    "romans\n",
    "forrest\n",
    "uz\n",
    "christus\n",
    "hanamel\n",
    "knapp\n",
    "hoar\n",
    "lithoman\n",
    "belarus\n",
    "hormos\n",
    "morales\n",
    "soleb\n",
    "stret\n",
    "armen\n",
    "sug\n",
    "julien\n",
    "mmh\n",
    "penh\n",
    "soberanes\n",
    "wolfensohn\n",
    "piraievs\n",
    "hooft\n",
    "sos\n",
    "ponton\n",
    "dement\n",
    "aires\n",
    "overy\n",
    "botanic\n",
    "natur\n",
    "lwin\n",
    "richter\n",
    "estefan\n",
    "btwc\n",
    "bartholomeus\n",
    "restad\n",
    "eskinder\n",
    "penn\n",
    "alecxandros\n",
    "griiz\n",
    "peanut\n",
    "fisher\n",
    "moussanett\n",
    "rimm\n",
    "eclecticollections\n",
    "deutscher\n",
    "iht\n",
    "golant\n",
    "unep\n",
    "amram\n",
    "osiris\n",
    "akkad\n",
    "mujer\n",
    "hoy\n",
    "löwenstein\n",
    "solin\n",
    "lenz\n",
    "youngs\n",
    "non\n",
    "russky\n",
    "avtar\n",
    "jessicah\n",
    "natalin\n",
    "mmm\n",
    "reyon\n",
    "yeimy\n",
    "gegen\n",
    "christentum\n",
    "heamọn\n",
    "monsen\n",
    "rodes\n",
    "lésvos\n",
    "ninsight\n",
    "bigras\n",
    "preiss\n",
    "christos\n",
    "morrow\n",
    "militọs\n",
    "ibal\n",
    "beavor\n",
    "demetrios\n",
    "madelin\n",
    "charpentier\n",
    "jacky\n",
    "ajocad\n",
    "ghantous\n",
    "mateus\n",
    "levering\n",
    "fuzel\n",
    "peary\n",
    "visigoth\n",
    "dikapọlis\n",
    "simottel\n",
    "munus\n",
    "círculos\n",
    "abuelos\n",
    "kọmashal\n",
    "léopold\n",
    "haịmenịọs\n",
    "niráklion\n",
    "szinger\n",
    "sunal\n",
    "soviak\n",
    "johanan\n",
    "pilatus\n",
    "tait\n",
    "heuvel\n",
    "simbal\n",
    "milagros\n",
    "progrès\n",
    "stiftung\n",
    "familien\n",
    "bibel\n",
    "nach\n",
    "deutschen\n",
    "uebersetzung\n",
    "padan\n",
    "kinaịt\n",
    "trahan\n",
    "dickey\n",
    "ọnan\n",
    "archeologist\n",
    "nanatọt\n",
    "vivat\n",
    "griffing\n",
    "finehas\n",
    "hauser\n",
    "woolley\n",
    "buchs\n",
    "midad\n",
    "žobrák\n",
    "siment\n",
    "efek\n",
    "rustad\n",
    "ephesos\n",
    "haug\n",
    "rainey\n",
    "nacional\n",
    "inọsh\n",
    "ressler\n",
    "alp\n",
    "béziers\n",
    "christiaan\n",
    "ewan\n",
    "doumen\n",
    "johnsson\n",
    "hermas\n",
    "aut\n",
    "thayers\n",
    "geh\n",
    "croisant\n",
    "ases\n",
    "stelter\n",
    "maccabaeus\n",
    "irwin\n",
    "mercony\n",
    "wridgway\n",
    "forier\n",
    "francesc\n",
    "kempis\n",
    "meyọn\n",
    "preussischer\n",
    "netherland\n",
    "haub\n",
    "schamp\n",
    "gustaw\n",
    "guoming\n",
    "kition\n",
    "evers\n",
    "soreg\n",
    "literal\n",
    "tavener\n",
    "iziọn\n",
    "bengal\n",
    "méndez\n",
    "verkhoyanskiy\n",
    "bibelforscher\n",
    "chicas\n",
    "campen\n",
    "gus\n",
    "yunaịted\n",
    "sevụnt\n",
    "akad\n",
    "luwis\n",
    "edrian\n",
    "mashbọn\n",
    "rozenstrọm\n",
    "sisk\n",
    "rosas\n",
    "bonel\n",
    "wiklif\n",
    "chals\n",
    "milet\n",
    "polley\n",
    "abon\n",
    "rumph\n",
    "halifaks\n",
    "jenney\n",
    "pesak\n",
    "biedl\n",
    "esan\n",
    "sabas\n",
    "flandas\n",
    "nahas\n",
    "bask\n",
    "chimiklis\n",
    "ravensbruk\n",
    "gramp\n",
    "bokas\n",
    "albenian\n",
    "hölterhoff\n",
    "peshian\n",
    "huth\n",
    "şirin\n",
    "héctor\n",
    "sommers\n",
    "napoléon\n",
    "nelsen\n",
    "willstätter\n",
    "józef\n",
    "tiv\n",
    "soas\n",
    "ginsburg\n",
    "rafel\n",
    "neic\n",
    "rimọn\n",
    "sigman\n",
    "kuhar\n",
    "oat\n",
    "teenager\n",
    "bonn\n",
    "pil\n",
    "ninjin\n",
    "steroid\n",
    "lickorish\n",
    "rooyen\n",
    "mutans\n",
    "brasil\n",
    "gerber\n",
    "bos\n",
    "dickmann\n",
    "nareọpagọs\n",
    "lüdemann\n",
    "womens\n",
    "childs\n",
    "žondor\n",
    "bangor\n",
    "sidney\n",
    "kurdas\n",
    "adelphos\n",
    "belzig\n",
    "indọljens\n",
    "leakey\n",
    "moskal\n",
    "och\n",
    "nexeter\n",
    "hemmig\n",
    "prostak\n",
    "wessels\n",
    "kuokkanen\n",
    "pilar\n",
    "ehud\n",
    "kues\n",
    "samuelsịn\n",
    "kep\n",
    "masachusets\n",
    "nan\n",
    "platt\n",
    "kastilian\n",
    "galishian\n",
    "ralf\n",
    "richad\n",
    "bangledesh\n",
    "nicolaus\n",
    "paulos\n",
    "yod\n",
    "ilias\n",
    "soris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neat version of the code without the tests\n",
    "\n",
    "# CHATGPT code to remove one_word occurence\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def remove_rare_words(corpus):\n",
    "    # Split the corpus into sentences\n",
    "    #sentences = re.split(r'[.!?]', corpus)\n",
    "    sentences = corpus.split('\\n')\n",
    "    \n",
    "    # Create a Counter object to count the occurrences of each word\n",
    "    word_counts = Counter()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.split():\n",
    "            word_counts[word] += 1\n",
    "            \n",
    "    #print (word_counts)\n",
    "    # Remove sentences that contain words that occur only once\n",
    "    #new_corpus = \"\"\n",
    "    new_corpus=[]\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        include_sentence = True\n",
    "        for word in sentence.split():\n",
    "            if word_counts[word] == 1:\n",
    "                include_sentence = False\n",
    "                break\n",
    "        if include_sentence:\n",
    "            #new_corpus += sentence + \" \" \n",
    "            new_corpus.append(sentence)\n",
    "    \n",
    "        \n",
    "    #return new_corpus.strip()\n",
    "    return \"\\n\".join(new_corpus),word_counts # returning the \"x\".strip().replace(\"  \",\" \") to remove extra spaces does not work when used within the function\n",
    "    \n",
    "\n",
    "#corpus = \"This is a sentence. This is another sentence. This is a unique sentence.\"\n",
    "#print(remove_rare_words(corpus))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
